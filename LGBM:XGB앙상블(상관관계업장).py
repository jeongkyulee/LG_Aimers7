# =========================================================
# LG Aimers â€” 7ì¼ ìˆ˜ìš”ì˜ˆì¸¡ (ë£° ì¤€ìˆ˜)
# ë² ì´ìŠ¤: ìŠ¤í…ë³„ LGB/XGB ì•™ìƒë¸” (ê°€ì¤‘ì¹˜ëŠ” Softmax(sMAPE) ê¸°ë°˜ ìë™ ì‚°ì¶œ)
# ì¶”ê°€(Step1): í™”ë‹´ìˆ²ì£¼ë§‰/í™”ë‹´ìˆ²ì¹´í˜ "ì „ìš©" LGB/XGB ëª¨ë¸ + ê³µìš© ì•™ìƒë¸”ê³¼ ë¸”ë Œë”©(Softmax(sMAPE))
# ì£¼ì˜: ì™¸ë¶€ í”¼ì²˜ ì¶”ê°€ ì—†ìŒ. ê¸°ì¡´ ê¸°ëŠ¥ ì‚­ì œ ì—†ìŒ. ì „ìš© ëª¨ë¸ì€ target log1p ì‚¬ìš©.
# =========================================================
import os, re, glob, warnings
warnings.filterwarnings("ignore")

import numpy as np
import pandas as pd
from sklearn.preprocessing import LabelEncoder
from sklearn.metrics import mean_absolute_error  # (í•™ìŠµ ì¤‘ early stoppingì€ MAE ê·¸ëŒ€ë¡œ ì‚¬ìš©)
from lightgbm import LGBMRegressor, early_stopping, log_evaluation
import xgboost as xgb
from workalendar.asia import SouthKorea

# ============ ì„¤ì • ============
LOOKBACK = 28
PREDICT_DAYS = 7
SEED = 42
np.random.seed(SEED)

SPECIAL_SHOPS = ["í™”ë‹´ìˆ²ì£¼ë§‰", "í™”ë‹´ìˆ²ì¹´í˜"]  # (ì›ë˜ í‹€ ìœ ì§€ìš© â€” ì•„ë˜ì—ì„œ ê·¸ë£¹ìœ¼ë¡œ ì¬ì •ì˜)
TEMP_SOFTMAX = 0.6  # Softmax temperature (0.3~1.0 ê¶Œì¥)

# ---- ê·¸ë£¹ ì •ì˜ (ìš”ì²­ ì‚¬í•­) ----
PAIR_1 = ["í™”ë‹´ìˆ²ì£¼ë§‰", "í™”ë‹´ìˆ²ì¹´í˜"]
PAIR_2 = ["ì¹´í˜í…Œë¦¬ì•„", "í¬ë ˆìŠ¤íŠ¸ë¦¿"]

# ============ sMAPE & Softmax ìœ í‹¸ ============
EPS = 1e-8

def smape(y_true, y_pred, eps: float = EPS, multiply_100: bool = False) -> float:
    yt = np.asarray(y_true, dtype=float).reshape(-1)
    yp = np.asarray(y_pred, dtype=float).reshape(-1)
    denom = np.abs(yt) + np.abs(yp) + eps
    val = np.mean(2.0 * np.abs(yp - yt) / denom)
    return (val * 100.0) if multiply_100 else val

def softmax_weights_from_smape(score_dict: dict, temperature: float = 1.0) -> dict:
    names = list(score_dict.keys())
    smapes = np.array([score_dict[n] for n in names], dtype=float)  # ë‚®ì„ìˆ˜ë¡ ì¢‹ìŒ
    tau = max(float(temperature), 1e-6)
    logits = -smapes / tau
    logits -= logits.max()
    w = np.exp(logits)
    w = w / (w.sum() + EPS)
    return dict(zip(names, w))

# ============ ê³µí†µ í•¨ìˆ˜/ë‹¬ë ¥ ============
cal = SouthKorea()

def get_holidays(years):
    return set(pd.Timestamp(d[0]) for y in years for d in cal.holidays(y))

HOLI_TRAIN = get_holidays([2023, 2024])
HOLI_TEST  = get_holidays([2024, 2025])

def get_season(m:int)->int:
    if m in [3,4,5]: return 0  # ë´„
    if m in [6,7,8]: return 1  # ì—¬ë¦„
    if m in [9,10,11]: return 2 # ê°€ì„
    return 3                   # ê²¨ìš¸

def preprocess_base(df: pd.DataFrame, *, use_test_holidays: bool=False) -> pd.DataFrame:
    """ê¸°ë³¸ ë‚ ì§œ/ë‹¬ë ¥/ì£¼ê¸°/ì—…ì¥/ë©”ë‰´ ë¶„ë¦¬ê¹Œì§€ í•œ ë²ˆì—."""
    df = df.copy()
    df['ë§¤ì¶œìˆ˜ëŸ‰'] = df['ë§¤ì¶œìˆ˜ëŸ‰'].clip(lower=0)
    df['ì˜ì—…ì¼ì'] = pd.to_datetime(df['ì˜ì—…ì¼ì'])
    df[['ì˜ì—…ì¥ëª…','ë©”ë‰´ëª…']] = df['ì˜ì—…ì¥ëª…_ë©”ë‰´ëª…'].str.split('_', n=1, expand=True)

    df['ìš”ì¼'] = df['ì˜ì—…ì¼ì'].dt.weekday
    df['ì›”']   = df['ì˜ì—…ì¼ì'].dt.month
    df['ì£¼ë§'] = df['ìš”ì¼'].isin([5,6]).astype(int)

    holi = HOLI_TEST if use_test_holidays else HOLI_TRAIN
    df['ê³µíœ´ì¼'] = df['ì˜ì—…ì¼ì'].isin(holi).astype(int)
    df['ê³„ì ˆ']  = df['ì›”'].apply(get_season)

    # ì£¼ê¸° ì¸ì½”ë”©
    df['ìš”ì¼_sin'] = np.sin(2*np.pi*df['ìš”ì¼']/7)
    df['ìš”ì¼_cos'] = np.cos(2*np.pi*df['ìš”ì¼']/7)
    df['ì›”_sin']   = np.sin(2*np.pi*df['ì›”']/12)
    df['ì›”_cos']   = np.cos(2*np.pi*df['ì›”']/12)
    df['ì—°ì¤‘ì¼']   = df['ì˜ì—…ì¼ì'].dt.dayofyear
    df['doy_sin']  = np.sin(2*np.pi*df['ì—°ì¤‘ì¼']/365.25)
    df['doy_cos']  = np.cos(2*np.pi*df['ì—°ì¤‘ì¼']/365.25)

    # ì „/í›„ ê³µíœ´ì¼
    cal_map = df[['ì˜ì—…ì¼ì','ê³µíœ´ì¼']].drop_duplicates().sort_values('ì˜ì—…ì¼ì')
    cal_map['ì „ì¼_ê³µíœ´ì¼'] = cal_map['ê³µíœ´ì¼'].shift(1).fillna(0).astype(int)
    cal_map['ìµì¼_ê³µíœ´ì¼'] = cal_map['ê³µíœ´ì¼'].shift(-1).fillna(0).astype(int)
    df = df.merge(cal_map[['ì˜ì—…ì¼ì','ì „ì¼_ê³µíœ´ì¼','ìµì¼_ê³µíœ´ì¼']], on='ì˜ì—…ì¼ì', how='left')

    # ì¼ë¶€ ì—…ì¥ ì„±ìˆ˜ê¸° í”Œë˜ê·¸
    peak_summer_shops = {'ëŠí‹°ë‚˜ë¬´ ì…€í”„BBQ'}
    peak_winter_shops = {'í¬ë ˆìŠ¤íŠ¸ë¦¿', 'í™”ë‹´ìˆ²ì£¼ë§‰', 'í™”ë‹´ìˆ²ì¹´í˜'}
    df['is_peak_summer_shop'] = df['ì˜ì—…ì¥ëª…'].isin(peak_summer_shops).astype(int)
    df['is_peak_winter_shop'] = df['ì˜ì—…ì¥ëª…'].isin(peak_winter_shops).astype(int)
    df['peak_summer'] = df['ì›”'].isin([6,7,8]).astype(int) * df['is_peak_summer_shop']
    df['peak_winter'] = df['ì›”'].isin([12,1,2]).astype(int) * df['is_peak_winter_shop']

    return df

# ì•µì»¤ ë©”ë‰´ ë§¤í•‘
ANCHORS = {
    'ëŠí‹°ë‚˜ë¬´ ì…€í”„BBQ': ['1ì¸ ìˆ˜ì €ì„¸íŠ¸', 'BBQ55'],
    'ë¯¸ë¼ì‹œì•„': ['ë¸ŒëŸ°ì¹˜', 'ë¸ŒëŸ°ì¹˜(ëŒ€ì¸)', 'ë¯¸ë¼ì‹œì•„ ë¸ŒëŸ°ì¹˜'],
    'ì—°íšŒì¥': ['ê³µê¹ƒë°¥', 'Cookie Platter'],
}

def is_anchor_menu(row):
    keys = ANCHORS.get(row['ì˜ì—…ì¥ëª…'], [])
    if not keys: return 0
    name = str(row['ë©”ë‰´ëª…'])
    return int(any(k in name for k in keys))

def consec_runs_of_zero(prev_series: pd.Series) -> pd.Series:
    out = np.zeros(len(prev_series), dtype=int)
    cnt = 0
    for i, v in enumerate(prev_series.fillna(-1).values):
        if v == 0: cnt += 1
        else: cnt = 0
        out[i] = cnt
    return pd.Series(out, index=prev_series.index)

def create_features(df: pd.DataFrame, le_menu: LabelEncoder, le_shop: LabelEncoder) -> pd.DataFrame:
    df = df.sort_values(['ì˜ì—…ì¥ëª…','ë©”ë‰´ëª…','ì˜ì—…ì¼ì']).copy()

    # ë¼ë²¨ ì¸ì½”ë”©
    if 'ë©”ë‰´ID' not in df.columns:
        df['ë©”ë‰´ID'] = le_menu.transform(df['ì˜ì—…ì¥ëª…_ë©”ë‰´ëª…'])
    if 'ì˜ì—…ì¥ID' not in df.columns:
        df['ì˜ì—…ì¥ID'] = le_shop.transform(df['ì˜ì—…ì¥ëª…'])

    # ---- ê¸°ë³¸ lag/rolling ----
    for lag in [1,2,7,14,28]:
        df[f'lag_{lag}'] = df.groupby('ë©”ë‰´ID')['ë§¤ì¶œìˆ˜ëŸ‰'].shift(lag)

    for win in [3,7,14]:
        g = df.groupby('ë©”ë‰´ID')['ë§¤ì¶œìˆ˜ëŸ‰'].shift(1).rolling(win)
        df[f'roll_mean_{win}'] = g.mean()
        df[f'roll_std_{win}']  = g.std()

    df['ewm_7'] = df.groupby('ë©”ë‰´ID')['ë§¤ì¶œìˆ˜ëŸ‰'].shift(1).ewm(span=7, adjust=False).mean()

    # ---- ìŠ¤íŒŒì´í¬ & ì—°ì† 0 ----
    prev = df.groupby('ë©”ë‰´ID')['ë§¤ì¶œìˆ˜ëŸ‰'].shift(1)
    q90  = prev.groupby(df['ë©”ë‰´ID']).transform(lambda s: s.rolling(28, min_periods=3).quantile(0.90))
    df['spike_prev'] = (prev > q90).astype(int)
    df['run_zero']   = prev.groupby(df['ë©”ë‰´ID']).transform(consec_runs_of_zero)

    # ---- ì—…ì¥ ì´ìˆ˜ìš” ----
    df['shop_total'] = df.groupby(['ì˜ì—…ì¥ID','ì˜ì—…ì¼ì'])['ë§¤ì¶œìˆ˜ëŸ‰'].transform('sum')
    grp_shop = df.groupby('ì˜ì—…ì¥ID', sort=False)
    df['shop_total_shift1'] = grp_shop['shop_total'].shift(1)
    for win in [7,14,28]:
        df[f'shop_roll_mean_{win}'] = grp_shop['shop_total_shift1'].transform(lambda s: s.rolling(win, min_periods=1).mean())
        df[f'shop_roll_std_{win}']  = grp_shop['shop_total_shift1'].transform(lambda s: s.rolling(win, min_periods=2).std())

    # ë©”ë‰´ ì ìœ ìœ¨
    df['share_7']  = df['roll_mean_7']  / (df['shop_roll_mean_7']  + 1e-6)
    df['share_14'] = df['roll_mean_14'] / (df['shop_roll_mean_14'] + 1e-6)
    df['share_28'] = df['roll_mean_14'] / (df['shop_roll_mean_28'] + 1e-6)

    # ---- ì•µì»¤ ë©”ë‰´ ----
    if 'is_anchor' not in df.columns:
        df['is_anchor'] = df.apply(is_anchor_menu, axis=1)
    df['anchor_sales'] = df['ë§¤ì¶œìˆ˜ëŸ‰'] * df['is_anchor']
    df['anchor_total'] = df.groupby(['ì˜ì—…ì¥ID','ì˜ì—…ì¼ì'])['anchor_sales'].transform('sum')
    df['anchor_total_shift1'] = grp_shop['anchor_total'].shift(1)
    for win in [7,14]:
        df[f'anchor_roll_{win}'] = grp_shop['anchor_total_shift1'].transform(lambda s: s.rolling(win, min_periods=1).mean())

    # ---- ì‹ ê·œ ì¶”ê°€: ìµœê·¼ í™œë™ì„± (ì£¼ë§‰/ì¹´í˜ ëŒ€ì‘) ----
    df['recent_activity'] = prev.groupby(df['ë©”ë‰´ID']).transform(lambda s: s.rolling(7, min_periods=1).apply(lambda r: (r>0).any(), raw=True))
    df['active_ratio']    = prev.groupby(df['ë©”ë‰´ID']).transform(lambda s: (s.rolling(28, min_periods=1).apply(lambda r: (r>0).sum(), raw=True))/28.0)

    # ---- ë³´ê°•: ì£¼ë§‰/ì¹´í˜ ëŒ€ì‘ í”¼ì²˜ ----
    df['is_open_7d'] = prev.groupby(df['ë©”ë‰´ID']).transform(
        lambda s: s.rolling(7, min_periods=1).max()
    )
    df['open_ratio_7d'] = prev.groupby(df['ë©”ë‰´ID']).transform(
        lambda s: s.rolling(7, min_periods=1).apply(lambda r: (r>0).sum(), raw=True) / 7.0
    )
    df['last_open_gap'] = prev.groupby(df['ë©”ë‰´ID']).transform(
        lambda s: s[::-1].groupby((s!=0)[::-1].cumsum()).cumcount()[::-1]
    )

    return df

# ============ ë°ì´í„° ë¡œë“œ & ì „ì²˜ë¦¬ ============
train = pd.read_csv("/Users/jeong-kyu/Documents/LG_Aimers_7ê¸°/open/train/train.csv")
train = preprocess_base(train, use_test_holidays=False)

# ì¸ì½”ë” í•™ìŠµ
le_menu = LabelEncoder().fit(train['ì˜ì—…ì¥ëª…_ë©”ë‰´ëª…'])
le_shop = LabelEncoder().fit(train['ì˜ì—…ì¥ëª…'])

# í”¼ì²˜ ìƒì„±
train_feat = create_features(train, le_menu, le_shop).dropna().reset_index(drop=True)

# ============ ê²€ì¦ ë¶„í• (ìµœê·¼ 60ì¼) ============
cutoff = train_feat['ì˜ì—…ì¼ì'].max() - pd.Timedelta(days=60)
train_df = train_feat[train_feat['ì˜ì—…ì¼ì'] <= cutoff].copy()
valid_df = train_feat[train_feat['ì˜ì—…ì¼ì'] >  cutoff].copy()

EXCLUDE = {
    'ì˜ì—…ì¼ì','ì˜ì—…ì¥ëª…_ë©”ë‰´ëª…','ì˜ì—…ì¥ëª…','ë©”ë‰´ëª…','ë§¤ì¶œìˆ˜ëŸ‰','target',
    'anchor_sales'  # ë‚´ë¶€ íŒŒìƒ(ëˆ„ìˆ˜ ì•„ë‹˜ì´ì§€ë§Œ ëª¨ë¸ì—” ë¶ˆí•„ìš”)
}
features = [c for c in train_feat.columns if c not in EXCLUDE]
print(f"ì‚¬ìš© í”¼ì²˜ ìˆ˜: {len(features)}")

# ============ ê³µìš© ëª¨ë¸ í•™ìŠµ + stepë³„ ê°€ì¤‘ (Softmax over sMAPE) ============
models_lgb, models_xgb = {}, {}
step_weights = {}   # step -> (wl, wx)  [LGB, XGB ê°€ì¤‘ì¹˜]

for step_ahead in range(1, PREDICT_DAYS+1):
    # íƒ€ê¹ƒ: step ahead
    tr = train_df.copy()
    tr['target'] = tr.groupby('ë©”ë‰´ID')['ë§¤ì¶œìˆ˜ëŸ‰'].shift(-step_ahead)
    tr = tr.dropna()
    X_train, y_train = tr[features], tr['target']

    va = valid_df.copy()
    va['target'] = va.groupby('ë©”ë‰´ID')['ë§¤ì¶œìˆ˜ëŸ‰'].shift(-step_ahead)
    va = va.dropna()
    X_val, y_val = va[features], va['target']

    # LGBM
    lgbm = LGBMRegressor(
        objective='regression',
        learning_rate=0.03,
        num_leaves=64,
        n_estimators=2000,
        subsample=0.8,
        colsample_bytree=0.8,
        random_state=SEED
    )
    lgbm.fit(
        X_train, y_train,
        eval_set=[(X_val, y_val)],
        eval_metric='mae',
        callbacks=[early_stopping(100), log_evaluation(200)]
    )
    models_lgb[step_ahead] = lgbm
    pred_lgb_va = lgbm.predict(X_val, num_iteration=getattr(lgbm, "best_iteration_", None))
    smape_l = smape(y_val, pred_lgb_va)

    # XGBoost
    dtrain = xgb.DMatrix(X_train, label=y_train)
    dval   = xgb.DMatrix(X_val,   label=y_val)
    params_xgb = dict(
        objective='reg:squarederror',
        eval_metric='mae',
        eta=0.03, max_depth=8,
        subsample=0.8, colsample_bytree=0.8,
        seed=SEED
    )
    xgbm = xgb.train(
        params_xgb, dtrain,
        num_boost_round=2000,
        evals=[(dtrain,'train'),(dval,'valid')],
        early_stopping_rounds=100,
        verbose_eval=200
    )
    models_xgb[step_ahead] = xgbm
    best_iter = getattr(xgbm, 'best_iteration', None)
    if best_iter is None:
        best_iter = getattr(xgbm, 'num_boosted_rounds', lambda: 2000)()
    pred_xgb_va = xgbm.predict(dval, iteration_range=(0, best_iter))
    smape_x = smape(y_val, pred_xgb_va)

    # stepë³„ ê°€ì¤‘ì¹˜: Softmax over sMAPE (ë‚®ì„ìˆ˜ë¡ ì¢‹ìŒ â†’ -sMAPEë¥¼ ë¡œì§“ìœ¼ë¡œ)
    w = softmax_weights_from_smape({"lgb": smape_l, "xgb": smape_x}, temperature=TEMP_SOFTMAX)
    wl, wx = float(w["lgb"]), float(w["xgb"])
    step_weights[step_ahead] = (wl, wx)
    print(f"[step={step_ahead}] sMAPE LGB={smape_l:.4f} | XGB={smape_x:.4f} -> wl={wl:.3f}, wx={wx:.3f}")

print("âœ… ëª¨ë“  Horizon ê³µìš© ëª¨ë¸ í•™ìŠµ ì™„ë£Œ")

# ================= ê³µìš© ì—…ì¥ë³„ Validation sMAPE ë¶„ì„ =================
valid_metrics = []
valid_preds_common = {}  # (step) -> DataFrame(index=va.index, columns=['pred_common'])

for step_ahead in range(1, PREDICT_DAYS+1):
    va = valid_df.copy()
    va['target'] = va.groupby('ë©”ë‰´ID')['ë§¤ì¶œìˆ˜ëŸ‰'].shift(-step_ahead)
    va = va.dropna()
    X_val, y_val = va[features], va['target']

    preds_lgb = models_lgb[step_ahead].predict(
        X_val, num_iteration=getattr(models_lgb[step_ahead], "best_iteration_", None)
    )
    xgbm = models_xgb[step_ahead]
    best_iter = getattr(xgbm, 'best_iteration', None)
    if best_iter is None:
        best_iter = getattr(xgbm, 'num_boosted_rounds', lambda: 2000)()
    preds_xgb = xgbm.predict(xgb.DMatrix(X_val), iteration_range=(0, best_iter))

    wl, wx = step_weights.get(step_ahead, (0.5,0.5))
    preds_common = wl*preds_lgb + wx*preds_xgb

    va = va.copy()
    va['pred_common'] = preds_common
    valid_preds_common[step_ahead] = va[['ì˜ì—…ì¥ëª…','ë©”ë‰´ID','target','pred_common']].reset_index(drop=True)

    shop_smape = va.groupby('ì˜ì—…ì¥ëª…').apply(lambda g: smape(g['target'], g['pred_common']))
    for shop, s in shop_smape.items():
        valid_metrics.append({'step': step_ahead, 'shop': shop, 'smape': s})

valid_metrics_df = pd.DataFrame(valid_metrics)
print(valid_metrics_df.pivot(index="shop", columns="step", values="smape").round(4))

# ============ (ì¶”ê°€) ì „ìš©/ê·¸ë£¹ ëª¨ë¸ í•™ìŠµ ============

# 1) ê·¸ë£¹ ì‚¬ì „ êµ¬ì„±: "PAIR_1", "PAIR_2", + ë‚˜ë¨¸ì§€ ì—…ì¥ì€ ê°ì ë‹¨ë… ê·¸ë£¹
GROUPS = {
    "PAIR_1": set(PAIR_1),
    "PAIR_2": set(PAIR_2),
}
_all_shops = set(train_df['ì˜ì—…ì¥ëª…'].unique().tolist())
_grouped = GROUPS["PAIR_1"] | GROUPS["PAIR_2"]
for s in sorted(_all_shops - _grouped):
    GROUPS[s] = {s}  # ë‹¨ë… ê·¸ë£¹

# (ì›ë˜ í‹€ ë³€ìˆ˜ëª… ì¬ì‚¬ìš©) SPECIAL_SHOPSë¥¼ "ê·¸ë£¹ í‚¤" ë¦¬ìŠ¤íŠ¸ë¡œ ëŒ€ì²´
SPECIAL_SHOPS = list(GROUPS.keys())

shop_models_lgb = {g:{} for g in SPECIAL_SHOPS}
shop_models_xgb = {g:{} for g in SPECIAL_SHOPS}
shop_step_blend = {g:{} for g in SPECIAL_SHOPS}        # ê³µìš© vs ì „ìš©(ê·¸ë£¹) ë¸”ë Œë”© ê°€ì¤‘
shop_internal_weights = {g:{} for g in SPECIAL_SHOPS}  # ì „ìš© ë‚´ë¶€ LGB/XGB Softmax ê°€ì¤‘ (wl_in, wx_in)

for group_key in SPECIAL_SHOPS:
    members = GROUPS[group_key]
    print(f"\nğŸ”§ ì „ìš©(ê·¸ë£¹) ëª¨ë¸ í•™ìŠµ: {group_key} | ë©¤ë²„: {sorted(members)}")
    # ê·¸ë£¹ ë°ì´í„°ë§Œ í•„í„°ë§
    tr_s = train_df[train_df['ì˜ì—…ì¥ëª…'].isin(members)].copy()
    va_s = valid_df[valid_df['ì˜ì—…ì¥ëª…'].isin(members)].copy()

    for step_ahead in range(1, PREDICT_DAYS+1):
        # íƒ€ê¹ƒ ì¤€ë¹„ (log1p)
        tr_s_ = tr_s.copy()
        tr_s_['target'] = tr_s_.groupby('ë©”ë‰´ID')['ë§¤ì¶œìˆ˜ëŸ‰'].shift(-step_ahead)
        tr_s_ = tr_s_.dropna()
        if len(tr_s_) == 0:
            continue
        X_tr_s, y_tr_s = tr_s_[features], np.log1p(tr_s_['target'].clip(lower=0))

        va_s_ = va_s.copy()
        va_s_['target'] = va_s_.groupby('ë©”ë‰´ID')['ë§¤ì¶œìˆ˜ëŸ‰'].shift(-step_ahead)
        va_s_ = va_s_.dropna()
        if len(va_s_) == 0:
            continue
        X_va_s, y_va_s = va_s_[features], np.log1p(va_s_['target'].clip(lower=0))
        y_va_true = va_s_['target'].values  # sMAPEëŠ” ì›ì²™ë„ë¡œ ê³„ì‚°

        # LGBM (ì „ìš©/ê·¸ë£¹)
        lgbm_s = LGBMRegressor(
            objective='regression',
            learning_rate=0.03,
            num_leaves=64,
            n_estimators=2000,
            subsample=0.8,
            colsample_bytree=0.8,
            random_state=SEED
        )
        lgbm_s.fit(
            X_tr_s, y_tr_s,
            eval_set=[(X_va_s, y_va_s)],
            eval_metric='mae',
            callbacks=[early_stopping(100), log_evaluation(200)]
        )
        shop_models_lgb[group_key][step_ahead] = lgbm_s

        # XGB (ì „ìš©/ê·¸ë£¹)
        dtr_s = xgb.DMatrix(X_tr_s, label=y_tr_s)
        dva_s = xgb.DMatrix(X_va_s, label=y_va_s)
        params_xgb_s = dict(
            objective='reg:squarederror',
            eval_metric='mae',
            eta=0.03, max_depth=8,
            subsample=0.8, colsample_bytree=0.8,
            seed=SEED
        )
        xgbm_s = xgb.train(
            params_xgb_s, dtr_s,
            num_boost_round=2000,
            evals=[(dtr_s,'train'),(dva_s,'valid')],
            early_stopping_rounds=100,
            verbose_eval=200
        )
        shop_models_xgb[group_key][step_ahead] = xgbm_s

        # ---- ì „ìš©(ê·¸ë£¹) ë‚´ë¶€ LGB/XGB Softmax ê°€ì¤‘ ----
        best_iter_s = getattr(xgbm_s, 'best_iteration', None)
        if best_iter_s is None:
            best_iter_s = getattr(xgbm_s, 'num_boosted_rounds', lambda: 2000)()

        preds_lgb_s_val_log = lgbm_s.predict(X_va_s, num_iteration=getattr(lgbm_s, "best_iteration_", None))
        preds_xgb_s_val_log = xgbm_s.predict(dva_s, iteration_range=(0, best_iter_s))
        pred_lgb_val = np.expm1(preds_lgb_s_val_log)  # ì›ì²™ë„
        pred_xgb_val = np.expm1(preds_xgb_s_val_log)  # ì›ì²™ë„

        s_lgb = smape(y_va_true, pred_lgb_val)
        s_xgb = smape(y_va_true, pred_xgb_val)
        w_in = softmax_weights_from_smape({"lgb": s_lgb, "xgb": s_xgb}, temperature=TEMP_SOFTMAX)
        wl_in, wx_in = float(w_in["lgb"]), float(w_in["xgb"])
        shop_internal_weights[group_key][step_ahead] = (wl_in, wx_in)

        preds_shop_val = wl_in*pred_lgb_val + wx_in*pred_xgb_val

        # ---- ê³µìš© vs ì „ìš©(ê·¸ë£¹) ë¸”ë Œë”© ê°€ì¤‘ (ì „ìš© ë¹„ì¤‘) ----
        va_common = valid_preds_common[step_ahead]
        common_for_group = va_common[va_common['ì˜ì—…ì¥ëª…'].isin(members)].reset_index(drop=True)

        n = min(len(common_for_group), len(preds_shop_val))
        if n == 0:
            continue
        tgt = common_for_group['target'].values[:n]
        pred_common = common_for_group['pred_common'].values[:n]
        pred_shop = preds_shop_val[:n]

        s_common = smape(tgt, pred_common)
        s_shop   = smape(tgt, pred_shop)
        w2 = softmax_weights_from_smape({"shop": s_shop, "common": s_common}, temperature=TEMP_SOFTMAX)
        w_shop = float(w2["shop"])  # ì „ìš©(ê·¸ë£¹) ë¹„ì¤‘
        shop_step_blend[group_key][step_ahead] = w_shop

        print(f"[{group_key} step={step_ahead}] ë‚´ë¶€ sMAPE LGB={s_lgb:.4f} XGB={s_xgb:.4f} -> wl_in={wl_in:.3f}, wx_in={wx_in:.3f} | "
              f"ê³µìš©vsì „ìš© sMAPE common={s_common:.4f} shop={s_shop:.4f} -> w_shop={w_shop:.3f}")

print("âœ… ì „ìš©(ê·¸ë£¹) ëª¨ë¸ í•™ìŠµ/ë‚´ë¶€ê°€ì¤‘/ë¸”ë Œë”© ê°€ì¤‘ ì‚°ì¶œ ì™„ë£Œ")

# ============ ì˜ˆì¸¡ í•¨ìˆ˜ ============
def predict_for_test(test_df: pd.DataFrame, prefix: str) -> pd.DataFrame:
    # ë™ì¼ ì „ì²˜ë¦¬ & ì¸ì½”ë”©
    test_df = preprocess_base(test_df, use_test_holidays=True)
    # ê° (ì˜ì—…ì¥ëª…_ë©”ë‰´ëª…) ìµœê·¼ 28ì¼ë§Œ ì‚¬ìš© (ë£° ì¤€ìˆ˜)
    test_df = test_df.sort_values(['ì˜ì—…ì¥ëª…_ë©”ë‰´ëª…','ì˜ì—…ì¼ì']).groupby('ì˜ì—…ì¥ëª…_ë©”ë‰´ëª…').tail(LOOKBACK)

    test_df['ë©”ë‰´ID']   = le_menu.transform(test_df['ì˜ì—…ì¥ëª…_ë©”ë‰´ëª…'])
    test_df['ì˜ì—…ì¥ID'] = le_shop.transform(test_df['ì˜ì—…ì¥ëª…'])

    df_feat = create_features(test_df, le_menu, le_shop).fillna(0)

    results = []
    df_pred = df_feat.copy()

    for step_ahead in range(1, PREDICT_DAYS+1):
        X_test = df_pred[features].fillna(0)

        # ê³µìš© ì˜ˆì¸¡
        preds_lgb = models_lgb[step_ahead].predict(
            X_test, num_iteration=getattr(models_lgb[step_ahead], "best_iteration_", None)
        )
        xgbm = models_xgb[step_ahead]
        best_iter = getattr(xgbm, 'best_iteration', None)
        if best_iter is None:
            best_iter = getattr(xgbm, 'num_boosted_rounds', lambda: 2000)()
        preds_xgb = xgbm.predict(xgb.DMatrix(X_test), iteration_range=(0, best_iter))

        wl, wx = step_weights.get(step_ahead, (0.5, 0.5))
        preds_common = wl*preds_lgb + wx*preds_xgb

        # ì „ìš©(ê·¸ë£¹) ì˜ˆì¸¡ + ê³µìš©ê³¼ ë¸”ë Œë”©
        preds_final = preds_common.copy()
        for group_key, members in GROUPS.items():
            mask = df_pred['ì˜ì—…ì¥ëª…'].isin(members)
            if not mask.any():
                continue
            has_lgb = (group_key in shop_models_lgb) and (step_ahead in shop_models_lgb[group_key])
            has_xgb = (group_key in shop_models_xgb) and (step_ahead in shop_models_xgb[group_key])
            if not (has_lgb and has_xgb):
                continue

            wl_in, wx_in = shop_internal_weights.get(group_key, {}).get(step_ahead, (0.5, 0.5))

            X_test_s = X_test[mask]
            lgbm_s = shop_models_lgb[group_key][step_ahead]
            xgbm_s = shop_models_xgb[group_key][step_ahead]
            best_iter_s = getattr(xgbm_s, 'best_iteration', None)
            if best_iter_s is None:
                best_iter_s = getattr(xgbm_s, 'num_boosted_rounds', lambda: 2000)()

            preds_lgb_s_log = lgbm_s.predict(X_test_s, num_iteration=getattr(lgbm_s, "best_iteration_", None))
            preds_xgb_s_log = xgbm_s.predict(xgb.DMatrix(X_test_s), iteration_range=(0, best_iter_s))
            preds_shop = wl_in*np.expm1(preds_lgb_s_log) + wx_in*np.expm1(preds_xgb_s_log)

            # ê³µìš© vs ì „ìš©(ê·¸ë£¹) ë¸”ë Œë”©
            w_shop = shop_step_blend.get(group_key, {}).get(step_ahead, 0.5)
            preds_final[mask] = w_shop*preds_shop + (1-w_shop)*preds_common[mask]

        # ë‹¤ìŒ stepì„ ìœ„í•´ "ì˜ˆì¸¡ì„ lag_{step}"ì— ë°˜ì˜
        df_pred[f'lag_{step_ahead}'] = preds_final

        # ê° ë©”ë‰´IDì˜ ë§ˆì§€ë§‰ í–‰ ê°’ â†’ ì œì¶œìš©
        for mid, g in df_pred.groupby('ë©”ë‰´ID'):
            pred_val = float(preds_final[g.index][-1])
            results.append({
                'ì˜ì—…ì¼ì': f"{prefix}+{step_ahead}ì¼",
                'ì˜ì—…ì¥ëª…_ë©”ë‰´ëª…': le_menu.inverse_transform([int(mid)])[0],
                'ë§¤ì¶œìˆ˜ëŸ‰': float(max(pred_val, 0.0))
            })

    return pd.DataFrame(results)

# ============ ì „ì²´ TEST ì˜ˆì¸¡/ì €ì¥ ============
all_preds = []
for path in sorted(glob.glob("/Users/jeong-kyu/Documents/LG_Aimers_7ê¸°/open/test/TEST_*.csv")):
    prefix = re.search(r'(TEST_\d+)', os.path.basename(path)).group(1)
    test_df = pd.read_csv(path)
    pred_df = predict_for_test(test_df, prefix)
    all_preds.append(pred_df)

full_pred_df = pd.concat(all_preds, ignore_index=True)

# ---- ì£¼ë§‰/ì¹´í˜ ë³´ì • (í›ˆë ¨ í‰ê·  vs ì˜ˆì¸¡ í‰ê·  ìŠ¤ì¼€ì¼ë§) ----
train_means = train.groupby("ì˜ì—…ì¥ëª…")["ë§¤ì¶œìˆ˜ëŸ‰"].mean()
pred_means  = full_pred_df.groupby(full_pred_df["ì˜ì—…ì¥ëª…_ë©”ë‰´ëª…"].str.split("_").str[0])["ë§¤ì¶œìˆ˜ëŸ‰"].mean()

for shop in ["í™”ë‹´ìˆ²ì£¼ë§‰", "í™”ë‹´ìˆ²ì¹´í˜"]:
    if shop in train_means.index and shop in pred_means.index:
        scale = train_means[shop] / (pred_means[shop] + 1e-6)
        full_pred_df.loc[full_pred_df["ì˜ì—…ì¥ëª…_ë©”ë‰´ëª…"].str.startswith(shop), "ë§¤ì¶œìˆ˜ëŸ‰"] *= scale
        print(f"[ë³´ì •] {shop} ìŠ¤ì¼€ì¼ {scale:.3f} ì ìš©")

submission = full_pred_df.pivot(index="ì˜ì—…ì¼ì",
                                columns="ì˜ì—…ì¥ëª…_ë©”ë‰´ëª…",
                                values="ë§¤ì¶œìˆ˜ëŸ‰").reset_index()

out_path = "/Users/jeong-kyu/Documents/LG_Aimers_7ê¸°/open/submission_plus_features_smape4(ìœ í˜•ì—…ì¥).csv"
submission.to_csv(out_path, index=False, encoding="utf-8-sig")
print("âœ… ì œì¶œ íŒŒì¼ ì €ì¥ ì™„ë£Œ:", out_path)
