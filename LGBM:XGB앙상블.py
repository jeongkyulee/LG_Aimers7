# =========================================================
# LG Aimers â€” 7ì¼ ìˆ˜ìš”ì˜ˆì¸¡ (ë£° ì¤€ìˆ˜)
# ë² ì´ìŠ¤: ìŠ¤í…ë³„ LGB/XGB ì•™ìƒë¸” (ê°€ì¤‘ì¹˜ëŠ” Softmax(sMAPE) ê¸°ë°˜ ìë™ ì‚°ì¶œ)
# ì¶”ê°€(Step1): í™”ë‹´ìˆ²ì£¼ë§‰/í™”ë‹´ìˆ²ì¹´í˜ "ì „ìš©" LGB/XGB ëª¨ë¸ + ê³µìš© ì•™ìƒë¸”ê³¼ ë¸”ë Œë”©(Softmax(sMAPE))
# ì£¼ì˜: ì™¸ë¶€ í”¼ì²˜ ì¶”ê°€ ì—†ìŒ. ê¸°ì¡´ ê¸°ëŠ¥ ì‚­ì œ ì—†ìŒ. ì „ìš© ëª¨ë¸ì€ target log1p ì‚¬ìš©.
# =========================================================
import os, re, glob, warnings
warnings.filterwarnings("ignore")

import numpy as np
import pandas as pd
from sklearn.preprocessing import LabelEncoder
from sklearn.metrics import mean_absolute_error  # (í•™ìŠµ ì¤‘ early stoppingì€ MAE ê·¸ëŒ€ë¡œ ì‚¬ìš©)
from lightgbm import LGBMRegressor, early_stopping, log_evaluation
import xgboost as xgb
from workalendar.asia import SouthKorea

# ============ ì„¤ì • ============
LOOKBACK = 28
PREDICT_DAYS = 7
SEED = 42
np.random.seed(SEED)

SPECIAL_SHOPS = ["í™”ë‹´ìˆ²ì£¼ë§‰", "í™”ë‹´ìˆ²ì¹´í˜"]  # ì „ìš© ëª¨ë¸ì„ í•™ìŠµí•  ì—…ì¥ë“¤

# [UPD: 6) stepë³„ Softmax temperature ìŠ¤ì¼€ì¤„]
# 1~2ì¼ì€ ê· ë“±í™”(ì˜¨ë„â†‘), í›„ë°˜ì€ ì„±ëŠ¥ ì¢‹ì€ ëª¨ë¸ì— ì§‘ì¤‘(ì˜¨ë„â†“)
TEMP_SOFTMAX_SCHEDULE = {1:1.1, 2:0.9, 3:0.7, 4:0.6, 5:0.5, 6:0.45, 7:0.4}
def temp_for_step(k:int) -> float:
    return float(TEMP_SOFTMAX_SCHEDULE.get(k, 0.6))

# [UPD: 5) ê¸€ë¡œë²Œ shop ì •ê·œí™” ë³´ì • ìŠ¤ìœ„ì¹˜ & í´ë¨í”„]
APPLY_GLOBAL_SHOP_NORMALIZATION = True
GLOBAL_NORM_MIN = 0.70
GLOBAL_NORM_MAX = 1.30

# [UPD: 4) ë¡¤ë§ ê²€ì¦ìš© ìœˆë„ ê¸¸ì´(ì¼)]
ROLLING_VAL_WINDOWS = [60, 45, 30]

# ============ sMAPE & Softmax ìœ í‹¸ ============
EPS = 1e-8

def smape(y_true, y_pred, eps: float = EPS, multiply_100: bool = False) -> float:
    yt = np.asarray(y_true, dtype=float).reshape(-1)
    yp = np.asarray(y_pred, dtype=float).reshape(-1)
    denom = np.abs(yt) + np.abs(yp) + eps
    val = np.mean(2.0 * np.abs(yp - yt) / denom)
    return (val * 100.0) if multiply_100 else val

def softmax_weights_from_smape(score_dict: dict, temperature: float = 1.0) -> dict:
    names = list(score_dict.keys())
    smapes = np.array([score_dict[n] for n in names], dtype=float)  # ë‚®ì„ìˆ˜ë¡ ì¢‹ìŒ
    tau = max(float(temperature), 1e-6)
    logits = -smapes / tau
    logits -= logits.max()
    w = np.exp(logits)
    w = w / (w.sum() + EPS)
    return dict(zip(names, w))

# ============ ê³µí†µ í•¨ìˆ˜/ë‹¬ë ¥ ============
cal = SouthKorea()

def get_holidays(years):
    return set(pd.Timestamp(d[0]) for y in years for d in cal.holidays(y))

HOLI_TRAIN = get_holidays([2023, 2024])
HOLI_TEST  = get_holidays([2024, 2025])

def get_season(m:int)->int:
    if m in [3,4,5]: return 0  # ë´„
    if m in [6,7,8]: return 1  # ì—¬ë¦„
    if m in [9,10,11]: return 2 # ê°€ì„
    return 3                   # ê²¨ìš¸

def preprocess_base(df: pd.DataFrame, *, use_test_holidays: bool=False) -> pd.DataFrame:
    """ê¸°ë³¸ ë‚ ì§œ/ë‹¬ë ¥/ì£¼ê¸°/ì—…ì¥/ë©”ë‰´ ë¶„ë¦¬ê¹Œì§€ í•œ ë²ˆì—."""
    df = df.copy()
    df['ë§¤ì¶œìˆ˜ëŸ‰'] = df['ë§¤ì¶œìˆ˜ëŸ‰'].clip(lower=0)
    df['ì˜ì—…ì¼ì'] = pd.to_datetime(df['ì˜ì—…ì¼ì'])
    df[['ì˜ì—…ì¥ëª…','ë©”ë‰´ëª…']] = df['ì˜ì—…ì¥ëª…_ë©”ë‰´ëª…'].str.split('_', n=1, expand=True)

    df['ìš”ì¼'] = df['ì˜ì—…ì¼ì'].dt.weekday
    df['ì›”']   = df['ì˜ì—…ì¼ì'].dt.month
    df['ì£¼ë§'] = df['ìš”ì¼'].isin([5,6]).astype(int)

    holi = HOLI_TEST if use_test_holidays else HOLI_TRAIN
    df['ê³µíœ´ì¼'] = df['ì˜ì—…ì¼ì'].isin(holi).astype(int)
    df['ê³„ì ˆ']  = df['ì›”'].apply(get_season)

    # ì£¼ê¸° ì¸ì½”ë”©
    df['ìš”ì¼_sin'] = np.sin(2*np.pi*df['ìš”ì¼']/7)
    df['ìš”ì¼_cos'] = np.cos(2*np.pi*df['ìš”ì¼']/7)
    df['ì›”_sin']   = np.sin(2*np.pi*df['ì›”']/12)
    df['ì›”_cos']   = np.cos(2*np.pi*df['ì›”']/12)
    df['ì—°ì¤‘ì¼']   = df['ì˜ì—…ì¼ì'].dt.dayofyear
    df['doy_sin']  = np.sin(2*np.pi*df['ì—°ì¤‘ì¼']/365.25)
    df['doy_cos']  = np.cos(2*np.pi*df['ì—°ì¤‘ì¼']/365.25)

    # ì „/í›„ ê³µíœ´ì¼
    cal_map = df[['ì˜ì—…ì¼ì','ê³µíœ´ì¼']].drop_duplicates().sort_values('ì˜ì—…ì¼ì')
    cal_map['ì „ì¼_ê³µíœ´ì¼'] = cal_map['ê³µíœ´ì¼'].shift(1).fillna(0).astype(int)
    cal_map['ìµì¼_ê³µíœ´ì¼'] = cal_map['ê³µíœ´ì¼'].shift(-1).fillna(0).astype(int)
    df = df.merge(cal_map[['ì˜ì—…ì¼ì','ì „ì¼_ê³µíœ´ì¼','ìµì¼_ê³µíœ´ì¼']], on='ì˜ì—…ì¼ì', how='left')

    # ì¼ë¶€ ì—…ì¥ ì„±ìˆ˜ê¸° í”Œë˜ê·¸
    peak_summer_shops = {'ëŠí‹°ë‚˜ë¬´ ì…€í”„BBQ'}
    peak_winter_shops = {'í¬ë ˆìŠ¤íŠ¸ë¦¿', 'í™”ë‹´ìˆ²ì£¼ë§‰', 'í™”ë‹´ìˆ²ì¹´í˜'}
    df['is_peak_summer_shop'] = df['ì˜ì—…ì¥ëª…'].isin(peak_summer_shops).astype(int)
    df['is_peak_winter_shop'] = df['ì˜ì—…ì¥ëª…'].isin(peak_winter_shops).astype(int)
    df['peak_summer'] = df['ì›”'].isin([6,7,8]).astype(int) * df['is_peak_summer_shop']
    df['peak_winter'] = df['ì›”'].isin([12,1,2]).astype(int) * df['is_peak_winter_shop']

    return df

# ì•µì»¤ ë©”ë‰´ ë§¤í•‘
ANCHORS = {
    'ëŠí‹°ë‚˜ë¬´ ì…€í”„BBQ': ['1ì¸ ìˆ˜ì €ì„¸íŠ¸', 'BBQ55'],
    'ë¯¸ë¼ì‹œì•„': ['ë¸ŒëŸ°ì¹˜', 'ë¸ŒëŸ°ì¹˜(ëŒ€ì¸)', 'ë¯¸ë¼ì‹œì•„ ë¸ŒëŸ°ì¹˜'],
    'ì—°íšŒì¥': ['ê³µê¹ƒë°¥', 'Cookie Platter'],
}

def is_anchor_menu(row):
    keys = ANCHORS.get(row['ì˜ì—…ì¥ëª…'], [])
    if not keys: return 0
    name = str(row['ë©”ë‰´ëª…'])
    return int(any(k in name for k in keys))

def consec_runs_of_zero(prev_series: pd.Series) -> pd.Series:
    out = np.zeros(len(prev_series), dtype=int)
    cnt = 0
    for i, v in enumerate(prev_series.fillna(-1).values):
        if v == 0: cnt += 1
        else: cnt = 0
        out[i] = cnt
    return pd.Series(out, index=prev_series.index)

# [UPD: 1) Feature ê°•í™” â€” longer lags / roll_28 / slope / interaction]
def _rolling_slope(arr: np.ndarray) -> float:
    # ì„ í˜•íšŒê·€ ê¸°ìš¸ê¸° (x=0..n-1)
    if len(arr) < 5 or np.all(np.isnan(arr)):
        return np.nan
    x = np.arange(len(arr))
    y = np.asarray(arr, dtype=float)
    mask = ~np.isnan(y)
    if mask.sum() < 5: return np.nan
    x = x[mask]; y = y[mask]
    if len(x) < 2: return np.nan
    # slope = cov(x,y)/var(x)
    vx = np.var(x)
    if vx == 0: return 0.0
    return float(np.cov(x, y, bias=True)[0,1] / vx)

def create_features(df: pd.DataFrame, le_menu: LabelEncoder, le_shop: LabelEncoder) -> pd.DataFrame:
    df = df.sort_values(['ì˜ì—…ì¥ëª…','ë©”ë‰´ëª…','ì˜ì—…ì¼ì']).copy()

    # ë¼ë²¨ ì¸ì½”ë”©
    if 'ë©”ë‰´ID' not in df.columns:
        df['ë©”ë‰´ID'] = le_menu.transform(df['ì˜ì—…ì¥ëª…_ë©”ë‰´ëª…'])
    if 'ì˜ì—…ì¥ID' not in df.columns:
        df['ì˜ì—…ì¥ID'] = le_shop.transform(df['ì˜ì—…ì¥ëª…'])

    # ---- ê¸°ë³¸/í™•ì¥ lag ----
    for lag in [1,2,7,14,28,35,56]:   # [UPD] 35, 56 ì¶”ê°€
        df[f'lag_{lag}'] = df.groupby('ë©”ë‰´ID')['ë§¤ì¶œìˆ˜ëŸ‰'].shift(lag)

    # ---- rolling ----
    for win in [3,7,14,28]:           # [UPD] roll_mean_28 ì¶”ê°€
        g = df.groupby('ë©”ë‰´ID')['ë§¤ì¶œìˆ˜ëŸ‰'].shift(1).rolling(win)
        df[f'roll_mean_{win}'] = g.mean()
        df[f'roll_std_{win}']  = g.std()

    df['ewm_7'] = df.groupby('ë©”ë‰´ID')['ë§¤ì¶œìˆ˜ëŸ‰'].shift(1).ewm(span=7, adjust=False).mean()

    # ---- ìŠ¤íŒŒì´í¬ & ì—°ì† 0 ----
    prev = df.groupby('ë©”ë‰´ID')['ë§¤ì¶œìˆ˜ëŸ‰'].shift(1)
    q90  = prev.groupby(df['ë©”ë‰´ID']).transform(lambda s: s.rolling(28, min_periods=3).quantile(0.90))
    df['spike_prev'] = (prev > q90).astype(int)
    df['run_zero']   = prev.groupby(df['ë©”ë‰´ID']).transform(consec_runs_of_zero)

    # ---- ì—…ì¥ ì´ìˆ˜ìš” ----
    df['shop_total'] = df.groupby(['ì˜ì—…ì¥ID','ì˜ì—…ì¼ì'])['ë§¤ì¶œìˆ˜ëŸ‰'].transform('sum')
    grp_shop = df.groupby('ì˜ì—…ì¥ID', sort=False)
    df['shop_total_shift1'] = grp_shop['shop_total'].shift(1)
    for win in [7,14,28]:
        df[f'shop_roll_mean_{win}'] = grp_shop['shop_total_shift1'].transform(lambda s: s.rolling(win, min_periods=1).mean())
        df[f'shop_roll_std_{win}']  = grp_shop['shop_total_shift1'].transform(lambda s: s.rolling(win, min_periods=2).std())

    # ë©”ë‰´ ì ìœ ìœ¨
    df['share_7']  = df['roll_mean_7']  / (df['shop_roll_mean_7']  + 1e-6)
    df['share_14'] = df['roll_mean_14'] / (df['shop_roll_mean_14'] + 1e-6)
    df['share_28'] = df['roll_mean_28'] / (df['shop_roll_mean_28'] + 1e-6)  # [UPD] ìˆ˜ì •

    # ---- ì•µì»¤ ë©”ë‰´ ----
    if 'is_anchor' not in df.columns:
        df['is_anchor'] = df.apply(is_anchor_menu, axis=1)
    df['anchor_sales'] = df['ë§¤ì¶œìˆ˜ëŸ‰'] * df['is_anchor']
    df['anchor_total'] = df.groupby(['ì˜ì—…ì¥ID','ì˜ì—…ì¼ì'])['anchor_sales'].transform('sum')
    df['anchor_total_shift1'] = grp_shop['anchor_total'].shift(1)
    for win in [7,14]:
        df[f'anchor_roll_{win}'] = grp_shop['anchor_total_shift1'].transform(lambda s: s.rolling(win, min_periods=1).mean())

    # ---- ìµœê·¼ í™œë™ì„± ----
    df['recent_activity'] = prev.groupby(df['ë©”ë‰´ID']).transform(lambda s: s.rolling(7, min_periods=1).apply(lambda r: (r>0).any(), raw=True))
    df['active_ratio']    = prev.groupby(df['ë©”ë‰´ID']).transform(lambda s: (s.rolling(28, min_periods=1).apply(lambda r: (r>0).sum(), raw=True))/28.0)

    # ---- ì˜¤í”ˆ/íœ´ì  íŒ¨í„´ ë³´ê°• ----
    df['is_open_7d'] = prev.groupby(df['ë©”ë‰´ID']).transform(lambda s: s.rolling(7, min_periods=1).max())
    df['open_ratio_7d'] = prev.groupby(df['ë©”ë‰´ID']).transform(lambda s: s.rolling(7, min_periods=1).apply(lambda r: (r>0).sum(), raw=True) / 7.0)
    df['last_open_gap'] = prev.groupby(df['ë©”ë‰´ID']).transform(lambda s: s[::-1].groupby((s!=0)[::-1].cumsum()).cumcount()[::-1])

    # ---- [UPD] ì¶”ì„¸(slope) í”¼ì²˜: 7/14ì¼
    df['slope_7']  = prev.groupby(df['ë©”ë‰´ID']).transform(lambda s: s.rolling(7, min_periods=5).apply(_rolling_slope, raw=True))
    df['slope_14'] = prev.groupby(df['ë©”ë‰´ID']).transform(lambda s: s.rolling(14, min_periods=7).apply(_rolling_slope, raw=True))

    # ---- [UPD] ì¸í„°ë™ì…˜: ì£¼ë§/ê³µíœ´ì¼/ì„±ìˆ˜ê¸° Ã— ì£¼ìš” lag
    for base in ['lag_1','lag_7','lag_14']:
        if base in df.columns:
            df[f'{base}_x_weekend']  = df[base] * df['ì£¼ë§']
            df[f'{base}_x_holiday']  = df[base] * df['ê³µíœ´ì¼']
            df[f'{base}_x_psummer']  = df[base] * df['peak_summer']
            df[f'{base}_x_pwinter']  = df[base] * df['peak_winter']

    return df

# ============ ë°ì´í„° ë¡œë“œ & ì „ì²˜ë¦¬ ============
train = pd.read_csv("/Users/jeong-kyu/Documents/LG_Aimers_7ê¸°/open/train/train.csv")
train = preprocess_base(train, use_test_holidays=False)

# ì¸ì½”ë” í•™ìŠµ
le_menu = LabelEncoder().fit(train['ì˜ì—…ì¥ëª…_ë©”ë‰´ëª…'])
le_shop = LabelEncoder().fit(train['ì˜ì—…ì¥ëª…'])

# í”¼ì²˜ ìƒì„±
train_feat = create_features(train, le_menu, le_shop).dropna().reset_index(drop=True)

# ============ ê²€ì¦ ë¶„í• (ìµœê·¼ 60ì¼) ============
cutoff = train_feat['ì˜ì—…ì¼ì'].max() - pd.Timedelta(days=60)
train_df = train_feat[train_feat['ì˜ì—…ì¼ì'] <= cutoff].copy()
valid_df = train_feat[train_feat['ì˜ì—…ì¼ì'] >  cutoff].copy()

EXCLUDE = {
    'ì˜ì—…ì¼ì','ì˜ì—…ì¥ëª…_ë©”ë‰´ëª…','ì˜ì—…ì¥ëª…','ë©”ë‰´ëª…','ë§¤ì¶œìˆ˜ëŸ‰','target',
    'anchor_sales'  # ë‚´ë¶€ íŒŒìƒ(ëˆ„ìˆ˜ ì•„ë‹˜ì´ì§€ë§Œ ëª¨ë¸ì—” ë¶ˆí•„ìš”)
}
features = [c for c in train_feat.columns if c not in EXCLUDE]
print(f"ì‚¬ìš© í”¼ì²˜ ìˆ˜: {len(features)}")

# ============ ê³µìš© ëª¨ë¸ í•™ìŠµ + stepë³„ ê°€ì¤‘ (Softmax over sMAPE) ============
models_lgb, models_xgb = {}, {}
step_weights = {}   # step -> (wl, wx)  [LGB, XGB ê°€ì¤‘ì¹˜]

for step_ahead in range(1, PREDICT_DAYS+1):
    # íƒ€ê¹ƒ: step ahead
    tr = train_df.copy()
    tr['target'] = tr.groupby('ë©”ë‰´ID')['ë§¤ì¶œìˆ˜ëŸ‰'].shift(-step_ahead)
    tr = tr.dropna()
    X_train, y_train = tr[features], tr['target']

    va = valid_df.copy()
    va['target'] = va.groupby('ë©”ë‰´ID')['ë§¤ì¶œìˆ˜ëŸ‰'].shift(-step_ahead)
    va = va.dropna()
    X_val, y_val = va[features], va['target']

    # [UPD: 3) ê³µìš© ëª¨ë¸ë„ log1pë¡œ í•™ìŠµ (í•™ìŠµ/ESëŠ” ë¡œê·¸ìŠ¤ì¼€ì¼, í‰ê°€ëŠ” expm1 í›„ sMAPE)]
    y_train_log = np.log1p(y_train.clip(lower=0))
    y_val_log   = np.log1p(y_val.clip(lower=0))

    # LGBM
    lgbm = LGBMRegressor(
        objective='regression',
        learning_rate=0.03,
        num_leaves=64,
        n_estimators=2500,          # [UPD] ì•½ê°„ ì¦ëŒ€
        subsample=0.8,
        colsample_bytree=0.75,      # [UPD] ì‚´ì§ ì¤„ì—¬ ê·œì œ ê°•í™”
        min_data_in_leaf=40,        # [UPD] ê·œì œ
        reg_lambda=1.0,             # [UPD] L2
        random_state=SEED
    )
    lgbm.fit(
        X_train, y_train_log,
        eval_set=[(X_val, y_val_log)],
        eval_metric='mae',          # ë¡œê·¸ìŠ¤ì¼€ì¼ MAEë¡œ ES
        callbacks=[early_stopping(120), log_evaluation(250)]
    )
    models_lgb[step_ahead] = lgbm
    pred_lgb_va_log = lgbm.predict(X_val, num_iteration=getattr(lgbm, "best_iteration_", None))
    pred_lgb_va = np.expm1(pred_lgb_va_log)
    smape_l = smape(y_val, pred_lgb_va)

    # XGBoost
    dtrain = xgb.DMatrix(X_train, label=y_train_log)
    dval   = xgb.DMatrix(X_val,   label=y_val_log)
    params_xgb = dict(
        objective='reg:squarederror',
        eval_metric='mae',    # ë¡œê·¸ìŠ¤ì¼€ì¼ MAE
        eta=0.03, max_depth=8,
        subsample=0.8, colsample_bytree=0.75,  # [UPD]
        gamma=0.0, reg_lambda=1.0,             # [UPD] ê·œì œ
        seed=SEED
    )
    xgbm = xgb.train(
        params_xgb, dtrain,
        num_boost_round=2500,                 # [UPD]
        evals=[(dtrain,'train'),(dval,'valid')],
        early_stopping_rounds=150,            # [UPD]
        verbose_eval=250
    )
    models_xgb[step_ahead] = xgbm
    best_iter = getattr(xgbm, 'best_iteration', None)
    if best_iter is None:
        best_iter = getattr(xgbm, 'num_boosted_rounds', lambda: 2500)()
    pred_xgb_va = np.expm1(xgbm.predict(dval, iteration_range=(0, best_iter)))
    smape_x = smape(y_val, pred_xgb_va)

    # [UPD: 4+6) ë¡¤ë§ ê²€ì¦ ê¸°ë°˜ + stepë³„ í…œí¼ëŸ¬ì²˜ë¡œ ê°€ì¤‘ ì‚°ì •]
    # ê¸°ë³¸ ë‹¨ì¼ ê²€ì¦ sMAPEë„ í¬í•¨í•˜ë˜, ë¡¤ë§ ìœˆë„ìš° í‰ê· ìœ¼ë¡œ ë³´ì •
    smapes_l, smapes_x = [smape_l], [smape_x]
    for win in ROLLING_VAL_WINDOWS:
        va_win = valid_df[valid_df['ì˜ì—…ì¼ì'] > (valid_df['ì˜ì—…ì¼ì'].max() - pd.Timedelta(days=win))].copy()
        va_win['target'] = va_win.groupby('ë©”ë‰´ID')['ë§¤ì¶œìˆ˜ëŸ‰'].shift(-step_ahead)
        va_win = va_win.dropna()
        if len(va_win) == 0: 
            continue
        X_w, y_w = va_win[features], va_win['target']
        # ì˜ˆì¸¡ (ë¡œê·¸â†’ì„ í˜•)
        lgb_p = np.expm1(models_lgb[step_ahead].predict(X_w, num_iteration=getattr(models_lgb[step_ahead], "best_iteration_", None)))
        xgb_p = np.expm1(models_xgb[step_ahead].predict(xgb.DMatrix(X_w), iteration_range=(0, best_iter)))
        smapes_l.append(smape(y_w, lgb_p))
        smapes_x.append(smape(y_w, xgb_p))

    sm_l = float(np.mean(smapes_l))
    sm_x = float(np.mean(smapes_x))

    w = softmax_weights_from_smape(
        {"lgb": sm_l, "xgb": sm_x},
        temperature=temp_for_step(step_ahead)
    )
    wl, wx = float(w["lgb"]), float(w["xgb"])
    step_weights[step_ahead] = (wl, wx)
    print(f"[step={step_ahead}] sMAPE LGB={sm_l:.4f} | XGB={sm_x:.4f} (avg over folds) -> wl={wl:.3f}, wx={wx:.3f}")

print("âœ… ëª¨ë“  Horizon ê³µìš© ëª¨ë¸ í•™ìŠµ ì™„ë£Œ")

# ================= ê³µìš© ì—…ì¥ë³„ Validation sMAPE ë¶„ì„ =================
valid_metrics = []
valid_preds_common = {}  # (step) -> DataFrame(index=va.index, columns=['pred_common'])

for step_ahead in range(1, PREDICT_DAYS+1):
    va = valid_df.copy()
    va['target'] = va.groupby('ë©”ë‰´ID')['ë§¤ì¶œìˆ˜ëŸ‰'].shift(-step_ahead)
    va = va.dropna()
    X_val, y_val = va[features], va['target']

    preds_lgb = np.expm1(models_lgb[step_ahead].predict(
        X_val, num_iteration=getattr(models_lgb[step_ahead], "best_iteration_", None)
    ))
    xgbm = models_xgb[step_ahead]
    best_iter = getattr(xgbm, 'best_iteration', None)
    if best_iter is None:
        best_iter = getattr(xgbm, 'num_boosted_rounds', lambda: 2500)()
    preds_xgb = np.expm1(xgbm.predict(xgb.DMatrix(X_val), iteration_range=(0, best_iter)))

    wl, wx = step_weights.get(step_ahead, (0.5,0.5))
    preds_common = wl*preds_lgb + wx*preds_xgb

    va = va.copy()
    va['pred_common'] = preds_common
    valid_preds_common[step_ahead] = va[['ì˜ì—…ì¥ëª…','ë©”ë‰´ID','target','pred_common']].reset_index(drop=True)

    shop_smape = va.groupby('ì˜ì—…ì¥ëª…').apply(lambda g: smape(g['target'], g['pred_common']))
    for shop, s in shop_smape.items():
        valid_metrics.append({'step': step_ahead, 'shop': shop, 'smape': s})

valid_metrics_df = pd.DataFrame(valid_metrics)
print(valid_metrics_df.pivot(index="shop", columns="step", values="smape").round(4))

# ============ (ì¶”ê°€) ì „ìš© ëª¨ë¸ í•™ìŠµ: SPECIAL_SHOPS ============
shop_models_lgb = {s:{} for s in SPECIAL_SHOPS}
shop_models_xgb = {s:{} for s in SPECIAL_SHOPS}
shop_step_blend = {s:{} for s in SPECIAL_SHOPS}   # ê³µìš© vs ì „ìš© ë¸”ë Œë”© ê°€ì¤‘(Softmax(sMAPE) ê¸°ë°˜, ì „ìš© ê°€ì¤‘ ë°˜í™˜)

for shop in SPECIAL_SHOPS:
    print(f"\nğŸ”§ ì „ìš© ëª¨ë¸ í•™ìŠµ: {shop}")
    # shop ë°ì´í„°ë§Œ í•„í„°ë§
    tr_s = train_df[train_df['ì˜ì—…ì¥ëª…']==shop].copy()
    va_s = valid_df[valid_df['ì˜ì—…ì¥ëª…']==shop].copy()

    for step_ahead in range(1, PREDICT_DAYS+1):
        # íƒ€ê¹ƒ ì¤€ë¹„ (log1p)
        tr_s_ = tr_s.copy()
        tr_s_['target'] = tr_s_.groupby('ë©”ë‰´ID')['ë§¤ì¶œìˆ˜ëŸ‰'].shift(-step_ahead)
        tr_s_ = tr_s_.dropna()
        if len(tr_s_) == 0:
            continue
        X_tr_s, y_tr_s = tr_s_[features], np.log1p(tr_s_['target'].clip(lower=0))

        va_s_ = va_s.copy()
        va_s_['target'] = va_s_.groupby('ë©”ë‰´ID')['ë§¤ì¶œìˆ˜ëŸ‰'].shift(-step_ahead)
        va_s_ = va_s_.dropna()
        X_va_s, y_va_s = va_s_[features], np.log1p(va_s_['target'].clip(lower=0))

        # LGBM (ì „ìš©)
        lgbm_s = LGBMRegressor(
            objective='regression',
            learning_rate=0.03,
            num_leaves=64,
            n_estimators=2500,         # [UPD]
            subsample=0.8,
            colsample_bytree=0.75,     # [UPD]
            min_data_in_leaf=40,       # [UPD]
            reg_lambda=1.0,            # [UPD]
            random_state=SEED
        )
        lgbm_s.fit(
            X_tr_s, y_tr_s,
            eval_set=[(X_va_s, y_va_s)],
            eval_metric='mae',
            callbacks=[early_stopping(120), log_evaluation(250)]
        )
        shop_models_lgb[shop][step_ahead] = lgbm_s

        # XGB (ì „ìš©)
        dtr_s = xgb.DMatrix(X_tr_s, label=y_tr_s)
        dva_s = xgb.DMatrix(X_va_s, label=y_va_s)
        params_xgb_s = dict(
            objective='reg:squarederror',
            eval_metric='mae',
            eta=0.03, max_depth=8,
            subsample=0.8, colsample_bytree=0.75,  # [UPD]
            gamma=0.0, reg_lambda=1.0,             # [UPD]
            seed=SEED
        )
        xgbm_s = xgb.train(
            params_xgb_s, dtr_s,
            num_boost_round=2500,                 # [UPD]
            evals=[(dtr_s,'train'),(dva_s,'valid')],
            early_stopping_rounds=150,            # [UPD]
            verbose_eval=250
        )
        shop_models_xgb[shop][step_ahead] = xgbm_s

        # ì „ìš© ë‚´ë¶€ ì•™ìƒë¸”(ì—¬ê¸´ 0.5/0.5 ìœ ì§€; í•„ìš”ì‹œ softmaxë¡œ í™•ì¥ ê°€ëŠ¥)
        best_iter_s = getattr(xgbm_s, 'best_iteration', None)
        if best_iter_s is None:
            best_iter_s = getattr(xgbm_s, 'num_boosted_rounds', lambda: 2500)()
        preds_lgb_s_val_log = lgbm_s.predict(X_va_s, num_iteration=getattr(lgbm_s, "best_iteration_", None))
        preds_xgb_s_val_log = xgbm_s.predict(dva_s, iteration_range=(0, best_iter_s))
        preds_shop_val = 0.5*np.expm1(preds_lgb_s_val_log) + 0.5*np.expm1(preds_xgb_s_val_log)

        # ê³µìš© ì˜ˆì¸¡ ëŒ€ë¹„ ì „ìš© ì˜ˆì¸¡ì˜ ë¸”ë Œë”© ê°€ì¤‘ ì‚°ì¶œ (Softmax over sMAPE, stepë³„ temp)
        va_common = valid_preds_common[step_ahead]
        common_for_shop = va_common[va_common['ì˜ì—…ì¥ëª…']==shop].reset_index(drop=True)

        n = min(len(common_for_shop), len(preds_shop_val))
        if n == 0:
            continue
        tgt = common_for_shop['target'].values[:n]
        pred_common = common_for_shop['pred_common'].values[:n]
        pred_shop = preds_shop_val[:n]

        s_common = smape(tgt, pred_common)
        s_shop   = smape(tgt, pred_shop)
        w2 = softmax_weights_from_smape(
            {"shop": s_shop, "common": s_common},
            temperature=temp_for_step(step_ahead)
        )
        w_shop = float(w2["shop"])  # ì „ìš© ê°€ì¤‘
        shop_step_blend[shop][step_ahead] = w_shop
        print(f"[{shop} step={step_ahead}] sMAPE common={s_common:.4f} | shop={s_shop:.4f} -> w_shop(ì „ìš©)={w_shop:.3f}")

print("âœ… ì „ìš© ëª¨ë¸ í•™ìŠµ/ë¸”ë Œë”© ê°€ì¤‘ ì‚°ì¶œ ì™„ë£Œ")

# ============ ì˜ˆì¸¡ í•¨ìˆ˜ ============
def predict_for_test(test_df: pd.DataFrame, prefix: str) -> pd.DataFrame:
    # ë™ì¼ ì „ì²˜ë¦¬ & ì¸ì½”ë”©
    test_df = preprocess_base(test_df, use_test_holidays=True)
    # ê° (ì˜ì—…ì¥ëª…_ë©”ë‰´ëª…) ìµœê·¼ 28ì¼ë§Œ ì‚¬ìš© (ë£° ì¤€ìˆ˜)
    test_df = test_df.sort_values(['ì˜ì—…ì¥ëª…_ë©”ë‰´ëª…','ì˜ì—…ì¼ì']).groupby('ì˜ì—…ì¥ëª…_ë©”ë‰´ëª…').tail(LOOKBACK)

    test_df['ë©”ë‰´ID']   = le_menu.transform(test_df['ì˜ì—…ì¥ëª…_ë©”ë‰´ëª…'])
    test_df['ì˜ì—…ì¥ID'] = le_shop.transform(test_df['ì˜ì—…ì¥ëª…'])

    df_feat = create_features(test_df, le_menu, le_shop).fillna(0)

    results = []
    df_pred = df_feat.copy()

    for step_ahead in range(1, PREDICT_DAYS+1):
        X_test = df_pred[features].fillna(0)

        # ê³µìš© ì˜ˆì¸¡ (ë¡œê·¸â†’ì„ í˜•)
        preds_lgb = np.expm1(models_lgb[step_ahead].predict(
            X_test, num_iteration=getattr(models_lgb[step_ahead], "best_iteration_", None)
        ))
        xgbm = models_xgb[step_ahead]
        best_iter = getattr(xgbm, 'best_iteration', None)
        if best_iter is None:
            best_iter = getattr(xgbm, 'num_boosted_rounds', lambda: 2500)()
        preds_xgb = np.expm1(xgbm.predict(xgb.DMatrix(X_test), iteration_range=(0, best_iter)))

        wl, wx = step_weights.get(step_ahead, (0.5, 0.5))
        preds_common = wl*preds_lgb + wx*preds_xgb

        # ì „ìš© ì˜ˆì¸¡(ì£¼ë§‰/ì¹´í˜ í–‰ë§Œ) + ê³µìš©ê³¼ ë¸”ë Œë”©
        preds_final = preds_common.copy()
        for shop in SPECIAL_SHOPS:
            mask = df_pred['ì˜ì—…ì¥ëª…'] == shop
            if not mask.any():
                continue
            if (shop not in shop_models_lgb) or (step_ahead not in shop_models_lgb[shop]):
                continue
            X_test_s = X_test[mask]
            # ì „ìš© ì˜ˆì¸¡(logâ†’exp)
            lgbm_s = shop_models_lgb[shop][step_ahead]
            xgbm_s = shop_models_xgb[shop][step_ahead]
            best_iter_s = getattr(xgbm_s, 'best_iteration', None)
            if best_iter_s is None:
                best_iter_s = getattr(xgbm_s, 'num_boosted_rounds', lambda: 2500)()
            preds_lgb_s_log = lgbm_s.predict(X_test_s, num_iteration=getattr(lgbm_s, "best_iteration_", None))
            preds_xgb_s_log = xgbm_s.predict(xgb.DMatrix(X_test_s), iteration_range=(0, best_iter_s))
            preds_shop = 0.5*np.expm1(preds_lgb_s_log) + 0.5*np.expm1(preds_xgb_s_log)

            w_shop = shop_step_blend.get(shop, {}).get(step_ahead, 0.5)
            preds_final[mask] = w_shop*preds_shop + (1-w_shop)*preds_common[mask]

        # ë‹¤ìŒ stepì„ ìœ„í•´ "ì˜ˆì¸¡ì„ lag_{step}"ì— ë°˜ì˜
        df_pred[f'lag_{step_ahead}'] = preds_final

        # ê° ë©”ë‰´IDì˜ ë§ˆì§€ë§‰ í–‰ ê°’ â†’ ì œì¶œìš©
        for mid, g in df_pred.groupby('ë©”ë‰´ID'):
            pred_val = float(preds_final[g.index][-1])
            results.append({
                'ì˜ì—…ì¼ì': f"{prefix}+{step_ahead}ì¼",
                'ì˜ì—…ì¥ëª…_ë©”ë‰´ëª…': le_menu.inverse_transform([int(mid)])[0],
                'ë§¤ì¶œìˆ˜ëŸ‰': max(int(round(pred_val)), 0)
            })

    return pd.DataFrame(results)

# ============ ì „ì²´ TEST ì˜ˆì¸¡/ì €ì¥ ============
all_preds = []
for path in sorted(glob.glob("/Users/jeong-kyu/Documents/LG_Aimers_7ê¸°/open/test/TEST_*.csv")):
    prefix = re.search(r'(TEST_\d+)', os.path.basename(path)).group(1)
    test_df = pd.read_csv(path)
    pred_df = predict_for_test(test_df, prefix)
    all_preds.append(pred_df)

full_pred_df = pd.concat(all_preds, ignore_index=True)

# ---- [UPD: 5) ê¸€ë¡œë²Œ shop ì •ê·œí™” (ì£¼ë§‰/ì¹´í˜ í¬í•¨ ì „ ì—…ì¥, ë³´ì • í­ í´ë¨í”„) ----
if APPLY_GLOBAL_SHOP_NORMALIZATION:
    train_means = train.groupby("ì˜ì—…ì¥ëª…")["ë§¤ì¶œìˆ˜ëŸ‰"].mean()
    pred_means  = full_pred_df.groupby(full_pred_df["ì˜ì—…ì¥ëª…_ë©”ë‰´ëª…"].str.split("_").str[0])["ë§¤ì¶œìˆ˜ëŸ‰"].mean()

    for shop, tr_mean in train_means.items():
        if shop in pred_means.index and tr_mean > 0:
            scale = float(tr_mean / (pred_means[shop] + 1e-6))
            # ê³¼ë³´ì • ë°©ì§€
            scale = max(GLOBAL_NORM_MIN, min(GLOBAL_NORM_MAX, scale))
            full_pred_df.loc[full_pred_df["ì˜ì—…ì¥ëª…_ë©”ë‰´ëª…"].str.startswith(shop), "ë§¤ì¶œìˆ˜ëŸ‰"] *= scale
            print(f"[ê¸€ë¡œë²Œ ë³´ì •] {shop} ìŠ¤ì¼€ì¼ {scale:.3f} ì ìš©")
else:
    # ê¸°ì¡´ ë‘ ì—…ì¥ë§Œ ë³´ì •(ì›ë˜ ì½”ë“œ ìœ ì§€)
    train_means = train.groupby("ì˜ì—…ì¥ëª…")["ë§¤ì¶œìˆ˜ëŸ‰"].mean()
    pred_means  = full_pred_df.groupby(full_pred_df["ì˜ì—…ì¥ëª…_ë©”ë‰´ëª…"].str.split("_").str[0])["ë§¤ì¶œìˆ˜ëŸ‰"].mean()
    for shop in ["í™”ë‹´ìˆ²ì£¼ë§‰", "í™”ë‹´ìˆ²ì¹´í˜"]:
        if shop in train_means.index and shop in pred_means.index:
            scale = train_means[shop] / (pred_means[shop] + 1e-6)
            full_pred_df.loc[full_pred_df["ì˜ì—…ì¥ëª…_ë©”ë‰´ëª…"].str.startswith(shop), "ë§¤ì¶œìˆ˜ëŸ‰"] *= scale
            print(f"[ë³´ì •] {shop} ìŠ¤ì¼€ì¼ {scale:.3f} ì ìš©")

# ì œì¶œ Pivot
submission = full_pred_df.pivot(index="ì˜ì—…ì¼ì",
                                columns="ì˜ì—…ì¥ëª…_ë©”ë‰´ëª…",
                                values="ë§¤ì¶œìˆ˜ëŸ‰").reset_index()

out_path = "/Users/jeong-kyu/Documents/LG_Aimers_7ê¸°/open/submission_plus_features_smape2(ê¸°ë²•ë³€ê²½).csv"
submission.to_csv(out_path, index=False, encoding="utf-8-sig")
print("âœ… ì œì¶œ íŒŒì¼ ì €ì¥ ì™„ë£Œ:", out_path)
